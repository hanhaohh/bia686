{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIA686 Lab Session 1: Natural Language Processing in Python\n",
    "Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The session is based on the Python programming language together with an open source library called the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is PIP\n",
    "pip is a package management system used to install and manage software packages written in Python. \n",
    "1. install a package: `pip install package-name`\n",
    "2. remove a package : `pip uninstall package-name`\n",
    "3. show installed packages: `pip install --upgrade package-name`\n",
    "\n",
    "### How to install PIP\n",
    "Installing with get-pip.py\n",
    "To install pip, securely download get-pip.py   https://bootstrap.pypa.io/get-pip.py\n",
    "\n",
    "Open your terminal(MAC) or CMD(Windows), then run the following:\n",
    "`python get-pip.py` \n",
    "\n",
    "PIP installed if there is no error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to install NLTK \n",
    "\n",
    "1. Mac: open the terminal and type : ```sudo pip install nltk```\n",
    "2. Windows: open the command line tool and type :  ```pip install nltk```\n",
    "\n",
    "Other library that you need: \n",
    "1. Numpy : This is a scientific computing library with support for multidimensional arrays and linear algebra, required for certain probability, tagging, clustering, and classification tasks.\n",
    "2. Matplotlib :  This is a 2D plotting library for data visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will be included in this session:\n",
    "1. String processing\ttokenize, stem\ttokenizers, sentence tokenizers, stemmers\n",
    "2. Part-of-speech tagging\ttag\tn-gram, backoff, Brill, HMM, TnT\n",
    "3. Machine learning\tclassify, cluster, tbl\tdecision tree, maximum entropy, naive Bayes, EM, k-means\n",
    "4. Chunking\tchunk\tregular expression, n-gram, named-entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#download the corpus and books\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Inaugural Address Corpus>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we can find the locations of a word in the text\n",
    "#This positional information can be displayed using a dispersion plot\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Statistics \n",
    "Frequency distribution,  it tells us the frequency of each vocabulary item in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145735"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of the words in this corpus\n",
    "len(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u',', 18713),\n",
       " (u'the', 13721),\n",
       " (u'.', 6862),\n",
       " (u'of', 6536),\n",
       " (u'and', 6024),\n",
       " (u'a', 4569),\n",
       " (u'to', 4542),\n",
       " (u';', 4072),\n",
       " (u'in', 3916),\n",
       " (u'that', 2982),\n",
       " (u\"'\", 2684),\n",
       " (u'-', 2552),\n",
       " (u'his', 2459),\n",
       " (u'it', 2209),\n",
       " (u'I', 2124),\n",
       " (u's', 1739),\n",
       " (u'is', 1695),\n",
       " (u'he', 1661),\n",
       " (u'with', 1659),\n",
       " (u'was', 1632),\n",
       " (u'as', 1620),\n",
       " (u'\"', 1478),\n",
       " (u'all', 1462),\n",
       " (u'for', 1414),\n",
       " (u'this', 1280),\n",
       " (u'!', 1269),\n",
       " (u'at', 1231),\n",
       " (u'by', 1137),\n",
       " (u'but', 1113),\n",
       " (u'not', 1103),\n",
       " (u'--', 1070),\n",
       " (u'him', 1058),\n",
       " (u'from', 1052),\n",
       " (u'be', 1030),\n",
       " (u'on', 1005),\n",
       " (u'so', 918),\n",
       " (u'whale', 906),\n",
       " (u'one', 889),\n",
       " (u'you', 841),\n",
       " (u'had', 767),\n",
       " (u'have', 760),\n",
       " (u'there', 715),\n",
       " (u'But', 705),\n",
       " (u'or', 697),\n",
       " (u'were', 680),\n",
       " (u'now', 646),\n",
       " (u'which', 640),\n",
       " (u'?', 637),\n",
       " (u'me', 627),\n",
       " (u'like', 624)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequency Distributions\n",
    "fdist1 = FreqDist(text1)\n",
    "fdist1.most_common(50)\n",
    "#fdist1.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inaugural.words()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the words America and citizen are used over time. The following code converts the words in the Inaugural corpus to lowercase using w.lower(), then checks if they start with either of the \"targets\" america or citizen using startswith(). Thus it will count words like American's and Citizens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1789', u'1793', u'1797', u'1801', u'1805', u'1809', u'1813', u'1817', u'1821', u'1825', u'1829', u'1833', u'1837', u'1841', u'1845', u'1849', u'1853', u'1857', u'1861', u'1865', u'1869', u'1873', u'1877', u'1881', u'1885', u'1889', u'1893', u'1897', u'1901', u'1905', u'1909', u'1913', u'1917', u'1921', u'1925', u'1929', u'1933', u'1937', u'1941', u'1945', u'1949', u'1953', u'1957', u'1961', u'1965', u'1969', u'1973', u'1977', u'1981', u'1985', u'1989', u'1993', u'1997', u'2001', u'2005', u'2009']\n"
     ]
    }
   ],
   "source": [
    "#Notice that the year of each text appears in its filename. To get the year out of the filename,\n",
    "#we extracted the first four characters, using fileid[:4].\n",
    "print [fileid[:4] for fileid in inaugural.fileids()]\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (target, fileid[:4])\n",
    "           for fileid in inaugural.fileids()\n",
    "           for w in inaugural.words(fileid)\n",
    "           for target in ['america', 'citizen']\n",
    "           if w.lower().startswith(target)) \n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stop words\n",
    "\n",
    "There is also a corpus of stopwords, that is, high-frequency words like the, to and also that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5229560503653893"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [w for w in text if w.lower() not in sw]\n",
    "len(content) / float(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### names\n",
    "Names corpus, containing 8,000 first names categorized by gender. The male and female names are stored in separate files. Let's find names which appear in both files, i.e. names that are ambiguous for gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'female.txt', u'male.txt']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = nltk.corpus.names\n",
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Abbey',\n",
       " u'Abbie',\n",
       " u'Abby',\n",
       " u'Addie',\n",
       " u'Adrian',\n",
       " u'Adrien',\n",
       " u'Ajay',\n",
       " u'Alex',\n",
       " u'Alexis',\n",
       " u'Alfie',\n",
       " u'Ali',\n",
       " u'Alix',\n",
       " u'Allie',\n",
       " u'Allyn',\n",
       " u'Andie',\n",
       " u'Andrea',\n",
       " u'Andy',\n",
       " u'Angel',\n",
       " u'Angie',\n",
       " u'Ariel',\n",
       " u'Ashley',\n",
       " u'Aubrey',\n",
       " u'Augustine',\n",
       " u'Austin',\n",
       " u'Averil',\n",
       " u'Barrie',\n",
       " u'Barry',\n",
       " u'Beau',\n",
       " u'Bennie',\n",
       " u'Benny',\n",
       " u'Bernie',\n",
       " u'Bert',\n",
       " u'Bertie',\n",
       " u'Bill',\n",
       " u'Billie',\n",
       " u'Billy',\n",
       " u'Blair',\n",
       " u'Blake',\n",
       " u'Bo',\n",
       " u'Bobbie',\n",
       " u'Bobby',\n",
       " u'Brandy',\n",
       " u'Brett',\n",
       " u'Britt',\n",
       " u'Brook',\n",
       " u'Brooke',\n",
       " u'Brooks',\n",
       " u'Bryn',\n",
       " u'Cal',\n",
       " u'Cam',\n",
       " u'Cammy',\n",
       " u'Carey',\n",
       " u'Carlie',\n",
       " u'Carlin',\n",
       " u'Carmine',\n",
       " u'Carroll',\n",
       " u'Cary',\n",
       " u'Caryl',\n",
       " u'Casey',\n",
       " u'Cass',\n",
       " u'Cat',\n",
       " u'Cecil',\n",
       " u'Chad',\n",
       " u'Chris',\n",
       " u'Chrissy',\n",
       " u'Christian',\n",
       " u'Christie',\n",
       " u'Christy',\n",
       " u'Clair',\n",
       " u'Claire',\n",
       " u'Clare',\n",
       " u'Claude',\n",
       " u'Clem',\n",
       " u'Clemmie',\n",
       " u'Cody',\n",
       " u'Connie',\n",
       " u'Constantine',\n",
       " u'Corey',\n",
       " u'Corrie',\n",
       " u'Cory',\n",
       " u'Courtney',\n",
       " u'Cris',\n",
       " u'Daffy',\n",
       " u'Dale',\n",
       " u'Dallas',\n",
       " u'Dana',\n",
       " u'Dani',\n",
       " u'Daniel',\n",
       " u'Dannie',\n",
       " u'Danny',\n",
       " u'Darby',\n",
       " u'Darcy',\n",
       " u'Darryl',\n",
       " u'Daryl',\n",
       " u'Deane',\n",
       " u'Del',\n",
       " u'Dell',\n",
       " u'Demetris',\n",
       " u'Dennie',\n",
       " u'Denny',\n",
       " u'Devin',\n",
       " u'Devon',\n",
       " u'Dion',\n",
       " u'Dionis',\n",
       " u'Dominique',\n",
       " u'Donnie',\n",
       " u'Donny',\n",
       " u'Dorian',\n",
       " u'Dory',\n",
       " u'Drew',\n",
       " u'Eddie',\n",
       " u'Eddy',\n",
       " u'Edie',\n",
       " u'Elisha',\n",
       " u'Emmy',\n",
       " u'Erin',\n",
       " u'Esme',\n",
       " u'Evelyn',\n",
       " u'Felice',\n",
       " u'Fran',\n",
       " u'Francis',\n",
       " u'Frank',\n",
       " u'Frankie',\n",
       " u'Franky',\n",
       " u'Fred',\n",
       " u'Freddie',\n",
       " u'Freddy',\n",
       " u'Gabriel',\n",
       " u'Gabriell',\n",
       " u'Gail',\n",
       " u'Gale',\n",
       " u'Gay',\n",
       " u'Gayle',\n",
       " u'Gene',\n",
       " u'George',\n",
       " u'Georgia',\n",
       " u'Georgie',\n",
       " u'Geri',\n",
       " u'Germaine',\n",
       " u'Gerri',\n",
       " u'Gerry',\n",
       " u'Gill',\n",
       " u'Ginger',\n",
       " u'Glen',\n",
       " u'Glenn',\n",
       " u'Grace',\n",
       " u'Gretchen',\n",
       " u'Gus',\n",
       " u'Haleigh',\n",
       " u'Haley',\n",
       " u'Hannibal',\n",
       " u'Harley',\n",
       " u'Hazel',\n",
       " u'Heath',\n",
       " u'Henrie',\n",
       " u'Hilary',\n",
       " u'Hillary',\n",
       " u'Holly',\n",
       " u'Ike',\n",
       " u'Ikey',\n",
       " u'Ira',\n",
       " u'Isa',\n",
       " u'Isador',\n",
       " u'Isadore',\n",
       " u'Jackie',\n",
       " u'Jaime',\n",
       " u'Jamie',\n",
       " u'Jan',\n",
       " u'Jean',\n",
       " u'Jere',\n",
       " u'Jermaine',\n",
       " u'Jerrie',\n",
       " u'Jerry',\n",
       " u'Jess',\n",
       " u'Jesse',\n",
       " u'Jessie',\n",
       " u'Jo',\n",
       " u'Jodi',\n",
       " u'Jodie',\n",
       " u'Jody',\n",
       " u'Joey',\n",
       " u'Jordan',\n",
       " u'Juanita',\n",
       " u'Jude',\n",
       " u'Judith',\n",
       " u'Judy',\n",
       " u'Julie',\n",
       " u'Justin',\n",
       " u'Karel',\n",
       " u'Kellen',\n",
       " u'Kelley',\n",
       " u'Kelly',\n",
       " u'Kelsey',\n",
       " u'Kerry',\n",
       " u'Kim',\n",
       " u'Kip',\n",
       " u'Kirby',\n",
       " u'Kit',\n",
       " u'Kris',\n",
       " u'Kyle',\n",
       " u'Lane',\n",
       " u'Lanny',\n",
       " u'Lauren',\n",
       " u'Laurie',\n",
       " u'Lee',\n",
       " u'Leigh',\n",
       " u'Leland',\n",
       " u'Lesley',\n",
       " u'Leslie',\n",
       " u'Lin',\n",
       " u'Lind',\n",
       " u'Lindsay',\n",
       " u'Lindsey',\n",
       " u'Lindy',\n",
       " u'Lonnie',\n",
       " u'Loren',\n",
       " u'Lorne',\n",
       " u'Lorrie',\n",
       " u'Lou',\n",
       " u'Luce',\n",
       " u'Lyn',\n",
       " u'Lynn',\n",
       " u'Maddie',\n",
       " u'Maddy',\n",
       " u'Marietta',\n",
       " u'Marion',\n",
       " u'Marlo',\n",
       " u'Martie',\n",
       " u'Marty',\n",
       " u'Mattie',\n",
       " u'Matty',\n",
       " u'Maurise',\n",
       " u'Max',\n",
       " u'Maxie',\n",
       " u'Mead',\n",
       " u'Meade',\n",
       " u'Mel',\n",
       " u'Meredith',\n",
       " u'Merle',\n",
       " u'Merrill',\n",
       " u'Merry',\n",
       " u'Meryl',\n",
       " u'Michal',\n",
       " u'Michel',\n",
       " u'Michele',\n",
       " u'Mickie',\n",
       " u'Micky',\n",
       " u'Millicent',\n",
       " u'Morgan',\n",
       " u'Morlee',\n",
       " u'Muffin',\n",
       " u'Nat',\n",
       " u'Nichole',\n",
       " u'Nickie',\n",
       " u'Nicky',\n",
       " u'Niki',\n",
       " u'Nikki',\n",
       " u'Noel',\n",
       " u'Ollie',\n",
       " u'Page',\n",
       " u'Paige',\n",
       " u'Pat',\n",
       " u'Patrice',\n",
       " u'Patsy',\n",
       " u'Pattie',\n",
       " u'Patty',\n",
       " u'Pen',\n",
       " u'Pennie',\n",
       " u'Penny',\n",
       " u'Perry',\n",
       " u'Phil',\n",
       " u'Pooh',\n",
       " u'Quentin',\n",
       " u'Quinn',\n",
       " u'Randi',\n",
       " u'Randie',\n",
       " u'Randy',\n",
       " u'Ray',\n",
       " u'Regan',\n",
       " u'Reggie',\n",
       " u'Rene',\n",
       " u'Rey',\n",
       " u'Ricki',\n",
       " u'Rickie',\n",
       " u'Ricky',\n",
       " u'Rikki',\n",
       " u'Robbie',\n",
       " u'Robin',\n",
       " u'Ronnie',\n",
       " u'Ronny',\n",
       " u'Rory',\n",
       " u'Ruby',\n",
       " u'Sal',\n",
       " u'Sam',\n",
       " u'Sammy',\n",
       " u'Sandy',\n",
       " u'Sascha',\n",
       " u'Sasha',\n",
       " u'Saundra',\n",
       " u'Sayre',\n",
       " u'Scotty',\n",
       " u'Sean',\n",
       " u'Shaine',\n",
       " u'Shane',\n",
       " u'Shannon',\n",
       " u'Shaun',\n",
       " u'Shawn',\n",
       " u'Shay',\n",
       " u'Shayne',\n",
       " u'Shea',\n",
       " u'Shelby',\n",
       " u'Shell',\n",
       " u'Shelley',\n",
       " u'Sibyl',\n",
       " u'Simone',\n",
       " u'Sonnie',\n",
       " u'Sonny',\n",
       " u'Stacy',\n",
       " u'Sunny',\n",
       " u'Sydney',\n",
       " u'Tabbie',\n",
       " u'Tabby',\n",
       " u'Tallie',\n",
       " u'Tally',\n",
       " u'Tammie',\n",
       " u'Tammy',\n",
       " u'Tate',\n",
       " u'Ted',\n",
       " u'Teddie',\n",
       " u'Teddy',\n",
       " u'Terri',\n",
       " u'Terry',\n",
       " u'Theo',\n",
       " u'Tim',\n",
       " u'Timmie',\n",
       " u'Timmy',\n",
       " u'Tobe',\n",
       " u'Tobie',\n",
       " u'Toby',\n",
       " u'Tommie',\n",
       " u'Tommy',\n",
       " u'Tony',\n",
       " u'Torey',\n",
       " u'Trace',\n",
       " u'Tracey',\n",
       " u'Tracie',\n",
       " u'Tracy',\n",
       " u'Val',\n",
       " u'Vale',\n",
       " u'Valentine',\n",
       " u'Van',\n",
       " u'Vin',\n",
       " u'Vinnie',\n",
       " u'Vinny',\n",
       " u'Virgie',\n",
       " u'Wallie',\n",
       " u'Wallis',\n",
       " u'Wally',\n",
       " u'Whitney',\n",
       " u'Willi',\n",
       " u'Willie',\n",
       " u'Willy',\n",
       " u'Winnie',\n",
       " u'Winny',\n",
       " u'Wynn']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "[w for w in male_names if w in female_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Raw Text\n",
    "\n",
    "The most important source of texts is undoubtedly the Web. It's convenient to have existing text collections to explore, such as the corpora we saw in the previous chapters. However, you probably have your own text sources in mind, and need to learn how to access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
    "response = urllib2.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176896"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize text into sentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize_list = sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent_tokenize uses an instance of PunktSentenceTokenizer from the nltk. tokenize.punkt module. This instance has already been trained on and works well for many European languages. So it knows what punctuation and characters mark the end of a sentence and the beginning of a new sentence.\n",
    "\n",
    "There are total 17 european languages that NLTK support for sentence tokenize, and you can use them as the following steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"this's a sent tokenize test.\",\n",
       " 'this is sent two.',\n",
       " 'is this sent three?',\n",
       " 'sent 4 is cool!',\n",
       " \"Now it's your turn.\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "text = \"this's a sent tokenize test. this is sent two. is this sent three? \\\n",
    "sent 4 is cool! Now it's your turn.\"\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\") #spanish.pickle\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola amigo.', 'Estoy bien.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_tokenizer = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n",
    "spanish_tokenizer.tokenize('Hola amigo. Estoy bien.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', \"'s\", 'a', 'test']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize text into words \n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('Hello World.')\n",
    "word_tokenize(\"this's a test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging \n",
    "Part-of-speech tagging is one of the most important text analysis tasks used to classify words into their part-of-speech and label them according the tagset which is a collection of tags used for the pos tagging. Part-of-speech tagging also known as word classes.\n",
    "\n",
    "Once you've tokenized the sentences you need to tag them. Tagging is not necessary for all purposes but it does help the computer better understand the objects and references in your sentences. Eg: NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dive', 'NNP'),\n",
       " ('into', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Part-of-speech', 'JJ'),\n",
       " ('tagging', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('POS', 'NNP'),\n",
       " ('Tagger', 'NNP')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.word_tokenize(\"Dive into NLTK: Part-of-speech tagging and POS Tagger\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming and Lemmatization are the basic text processing methods for English text. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. \n",
    "\n",
    "Stemming the process by which endings are removed from words in order to remove things like tense or plurality. It's not appropriate for all cases but can make it easier to connect together tenses to see if you're covering the same subject matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pstemmer = nltk.PorterStemmer()\n",
    "lstemmer = nltk.LancasterStemmer()\n",
    "wnlemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "pstemmer.stem('maximum')\n",
    "lstemmer.stem('maximum')\n",
    "pstemmer.stem('presumably')\n",
    "lstemmer.stem('presumably')\n",
    "pstemmer.stem('saying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem\n",
      "algorithm\n",
      "attempt\n",
      "to\n",
      "automat\n",
      "remov\n",
      "suffix\n",
      "(\n",
      "and\n",
      "in\n",
      "some\n",
      "case\n",
      "prefix\n",
      ")\n",
      "in\n",
      "order\n",
      "to\n",
      "find\n",
      "the\n",
      "``\n",
      "root\n",
      "word\n",
      "''\n",
      "or\n",
      "stem\n",
      "of\n",
      "a\n",
      "given\n",
      "word\n",
      ".\n",
      "Thi\n",
      "is\n",
      "use\n",
      "in\n",
      "variou\n",
      "natur\n",
      "languag\n",
      "process\n",
      "scenario\n",
      ",\n",
      "such\n",
      "as\n",
      "search\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "new_text = \"\"\"Stemming algorithms attempt to automatically remove suffixes (and in some \n",
    "cases prefixes) in order to find the \"root word\" or stem of a given word. This \n",
    "is useful in various natural language processing scenarios, such as search \"\"\"\n",
    "ps = PorterStemmer()\n",
    "from nltk import word_tokenize\n",
    "for i in word_tokenize(new_text):\n",
    "    print ps.stem(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good morning how are you doing today the waiter said'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import sent_tokenize\n",
    "sentence = 'good morning, how are you doing today\". the waiter said'\n",
    "exclude = set(string.punctuation)\n",
    "sentence_without_punc = ''.join(ch for ch in sentence if ch not in exclude)\n",
    "sentence_without_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### remove_non_ascii_2\n",
    "import re\n",
    "def remove_non_ascii_2(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]',' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "Chunking grabs chunks of text that might be more meaningful to your research or program. You create a list of parts of speech and run that over your corpus. It will extract the phrasing that you need.\n",
    "\n",
    "Remember you've got to customize it to the part of speech tagger that you're using, like Brown or the Stanford Tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#name recognition in python\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import LineTokenizer\n",
    "from nltk.corpus import state_union\n",
    "fw = open(\"247/064.txt\")\n",
    "text = remove_non_ascii_2(fw.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Jersey Medical School']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Lee J. Brooks']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-b89dfc7077ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#   words = [word for word in words if word.lower() not in string.punctuation]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msentt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mperson_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mw_td_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_td_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_short_binstring\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_short_binstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0mlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSHORT_BINSTRING\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_short_binstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sents = LineTokenizer( blanklines=u'discard').tokenize(text.strip())\n",
    "for i in sents:\n",
    "    words = nltk.word_tokenize(i)\n",
    "#   words = [word for word in words if word.lower() not in string.punctuation]\n",
    "    pos = nltk.pos_tag(words)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "    person_list = []\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "    print person_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "Text Classification is very useful technique in text analysis, such as it can be used in spam filtering, language identification, sentiment analysis, genre classification and etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(names)\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Webb', 'male'),\n",
       " (u'Casper', 'male'),\n",
       " (u'Arne', 'male'),\n",
       " (u'Carine', 'female'),\n",
       " (u'Ginger', 'male'),\n",
       " (u'Jewell', 'female'),\n",
       " (u'Adrian', 'female'),\n",
       " (u'Florri', 'female'),\n",
       " (u'Prince', 'male'),\n",
       " (u'Jabez', 'male')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing for a text classifier is feature, which can be very flexible, and defined by human engineer. Here, we just use the final letter of a given name as the feature, and build a dictionary containing relevant information about a given name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'y'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('Gary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(gender_features(n), g) for (n, g) in names]\n",
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'last_letter': u'b'}, 'male'),\n",
       " ({'last_letter': u'r'}, 'male'),\n",
       " ({'last_letter': u'e'}, 'male'),\n",
       " ({'last_letter': u'e'}, 'female'),\n",
       " ({'last_letter': u'r'}, 'male'),\n",
       " ({'last_letter': u'l'}, 'female'),\n",
       " ({'last_letter': u'n'}, 'female'),\n",
       " ({'last_letter': u'i'}, 'female'),\n",
       " ({'last_letter': u'e'}, 'male'),\n",
       " ({'last_letter': u'z'}, 'male')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[500:], featuresets[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "nb_classifier = NaiveBayesClassifier.train(train_set)\n",
    "nb_classifier.classify(gender_features('Gary'))\n",
    "nb_classifier.classify(gender_features('Grace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import classify\n",
    "classify.accuracy(nb_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = u'a'           female : male   =     37.0 : 1.0\n",
      "             last_letter = u'k'             male : female =     32.3 : 1.0\n",
      "             last_letter = u'f'             male : female =     15.3 : 1.0\n",
      "             last_letter = u'p'             male : female =     12.6 : 1.0\n",
      "             last_letter = u'd'             male : female =     10.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
